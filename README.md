# Causal Integration in Foundation Models for Mechanistic Understanding 

## Challenge 

Despite their enormous popularity, the mechanisms that LLMs use to perform tasks and the related failure modes are poorly understood. While LLMs demonstrate strong reasoning capabilities across a wide range of tasks, their internal decision-making process often remains opaque, raising challenges in ensuring robustness and reliability of the model predictions, or their alignment with desirable overarching principles. Recent work has proposed causal mediation analysis to trace how specific model components transmit information in arithmetic reasoning, and attention-based probes to uncover reasoning trees in multi-step reasoning tasks. Other approaches interpret self-attention through structural causal models, or leverage diffusion models for scalable causal discovery. Building on these insights, the project will use causal frameworks to identify which neurons, circuits, or attention heads mediate predictions, and how information flows across layers. By grounding mechanistic interpretability in causal inference, the project aims to move beyond surface-level correlations and provide principled, intervention-based explanations of model behavior, contributing to safer and more robust AI systems. 

## Project Aims 

- To investigate how causal inference can be applied to mechanistic interpretability of large language models (LLMs) or diffusion models. 
- To evaluate the strengths and limitations of causal mediation, attention probes, and similar approaches in uncovering internal model mechanisms. 
- Gain technical familiarity with analyzing the flow of information across neural layers during prediction tasks. 
- Initiate the development and implementation of alternative approaches to leveraging causal principles to mechanistic interpretability. 

## Learning Objectives 

By the end of the project, students will be able to: 

- Conduct a systematic literature review of existing or related approaches for addressing the challenge. 
- Gain foundational understanding of mechanistic interpretability and causal inference methods. 
- Learn to apply tools such as causal mediation analysis, attention-based probing, and diffusion-based causal discovery to model internals. 
- Develop skills in designing controlled interventions and interpreting their effects on model predictions. 
- Critically assess current research and identify gaps in causal interpretability approaches. 

## Learning Outcomes 

Upon successful completion of this project, students will be able to: 

- Articulate the theoretical rationale for employing causal inference in mechanistic interpretability research. 
- Formulate and empirically test hypotheses regarding mechanisms underpinning LLM / diffusion model predictions. 
- Demonstrate the ability to map and evaluate causal dependencies among internal model components. 
- Compare and contrast multiple causal interpretability techniques, providing evidence-based justifications for methodological choices. 

## Skills Required/Prerequisites 

- Deep learning fundamentals and experience with neural network inference. 
- Python programming proficiency with ML frameworks (PyTorch/TensorFlow). 
- A basic understanding of causal inference concepts. 

## Skills to be developed and supporting learning 

- Advanced understanding of causal inference. 
- Experience with different approaches to mechanistic interpretability using open-source models. 
- Group software development, workflows and good engineering practices.  
- Experimental investigations and documentation. 
- Familiarity with large-scale data processing and computational resources. 
  
## Support Structures 

|Role|Person(s)|
|----|---------|
|University project mentor | Yarin Gal |
|University research mentor | tbc |
|University Research Software Engineer | to be assigned |
|EIT project mentor | Leonardo Cotta |

## Relevant Background Reading 

|Title|Link|
|-----|----|
|Diffusion Models for Causal Discovery via Topological Ordering|https://arxiv.org/abs/2210.06201|  
|Causal Diffusion Transformers for Generative Modeling  |https://arxiv.org/abs/2412.12095|
|Causal Interpretation of Self-Attention in Pre-Trained Transformers |https://arxiv.org/abs/2310.20307|
|Towards Uncovering How Large Language Model Works: An|https://arxiv.org/abs/2402.10688|
|Towards a Mechanistic Interpretation of Multi-Step Reasoning|https://arxiv.org/abs/2310.14491|  
|Finding Alignments Between Interpretable Causal Variables and| https://arxiv.org/abs/2303.02536 |
|A Mechanistic Interpretation of Arithmetic Reasoning in Language| https://arxiv.org/abs/2305.15054 |
 
