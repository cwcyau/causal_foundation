# Causal Integration in Foundation Models for Mechanistic Understanding 


## Challenge

Despite their enormous popularity, the mechanisms that LLMs and other foundation models use to perform tasks and the related failure modes are poorly understood. While LLMs demonstrate strong reasoning capabilities across a wide range of tasks, their internal decision-making process often remains opaque, raising challenges in ensuring robustness and reliability of the model predictions, or their alignment with desirable overarching principles. These issues have become even more apparent recently as LLMs now support high-stakes, causal decisions in healthcare and policy. Thus, in this project we ask the question: How can we better understand the causal mechanisms behind black-box, foundation model predictions?

The connections between  causal inference and attribution methods like Shapley values, root cause identification approaches and sensitivity analysis have already been explored in the literature. Furthermore, recent representation learning work proposes to uncover the mechanisms supporting predictions in foundation models using causal mediation analysis to trace information flow in arithmetic reasoning and attention-based probes to reveal reasoning trees in multi-step tasks. This project will develop principled approaches grounded in causal inference to identify mechanisms, e.g., neurons, circuits, or attention heads that mediate predictions and trace how information flows across layers. By grounding mechanistic interpretability in causal inference, we move beyond correlations to provide principled, intervention-based explanations of model behavior in sensitive decision-making scenarios.

## Project Aims

- To investigate how existing causal inference methodology can be used to support mechanistic interpretability of foundation models, such as large language models (LLMs) or diffusion models.

- To evaluate the strengths and limitations of causal mediation, attention probes, and similar approaches in uncovering internal model mechanisms.

- Gain technical familiarity with analyzing the flow of information across neural layers during prediction tasks.

- Initiate the development and implementation of alternative approaches to leveraging causal principles to mechanistic interpretability.

## Learning Objectives

By the end of the project, students will be able to:

- Conduct a systematic literature review of existing or related approaches for addressing the challenge.

- Gain foundational understanding of mechanistic interpretability and causal inference methods.

- Learn to apply tools such as causal mediation analysis, attention-based probing, and diffusion-based causal discovery to model internals.

- Develop skills in designing controlled interventions and interpreting their effects on model predictions.

- Critically assess current research and identify gaps in causal interpretability approaches.

## Learning Outcomes

Upon successful completion of this project, students will be able to:

- Articulate the theoretical rationale for employing causal inference in mechanistic interpretability research.

- Formulate and empirically test hypotheses regarding mechanisms underpinning LLM / diffusion model predictions.

- Demonstrate the ability to map and evaluate causal dependencies among internal model components.

- Compare and contrast multiple causal interpretability techniques, providing evidence-based justifications for methodological choices.

## Skills Required/Prerequisites

- Deep learning fundamentals and experience with neural network inference.

- Python programming proficiency with ML frameworks (PyTorch/TensorFlow).

- A basic understanding of causal inference concepts.

## Skills to be developed and supporting learning

- Advanced understanding of causal inference.

- Experience with different approaches to mechanistic interpretability using open-source models.

- Group software development, workflows and good engineering practices. 

- Experimental investigations and documentation.

- Familiarity with large-scale data processing and computational resources.

## Support

|Role|Person(s)|
|----|---------|
|University project mentor | tbc |
|University project mentor | tbc|
|University research mentor | tbc |
|University Research Software Engineer | to be assigned |
|EIT project mentor | Leonardo Cotta |

## Relevant Background Reading

https://arxiv.org/abs/2210.06201 

https://arxiv.org/abs/2412.12095 

https://arxiv.org/abs/2310.20307

https://arxiv.org/abs/2402.10688 

https://arxiv.org/abs/2310.14491 

https://arxiv.org/abs/2303.02536 

https://arxiv.org/abs/2305.15054  

https://arxiv.org/abs/2004.12265 

 https://arxiv.org/abs/2106.02997 

https://openreview.net/forum?id=PlT25DHV8J
